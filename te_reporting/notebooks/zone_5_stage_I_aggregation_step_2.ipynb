{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zone 5 stage I aggregation step 2\n",
    "Task description\tAutomation\n",
    "After aggregating data to the day-level, create a table with row-wise listing of:\t\n",
    "1. Event column name\tTable1\n",
    "2. Total number of events for column over total number of events for ALL columns\tTable1\n",
    "\t\n",
    "Rationale: Proportion of events for a particular event column. Expected to be similar to zone 4 proportions\t\n",
    "\n",
    "Table 1\t\n",
    "\t\n",
    "\tNum events / Total num events \n",
    "Column 1\t2%\n",
    "Column 2\t5%\n",
    "Column 3\t3%\n",
    "Column 4\t5%\n",
    "Column 5\t1%\n",
    "…\t…\n",
    "Example \n",
    "Date  \tPurchase\tCalls\tMoney spend\n",
    "1/1/00\t0\t3\t50\n",
    "1/2/00\t1\t0\t51\n",
    "1/3/00\t0\t4\t52\n",
    "1/4/00\t0\t0\t32\n",
    "1/5/00\t1\t1\t0\n",
    "\t   0.33\t0.6\t0.80\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sys import argv\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from helper_functions import get_imputed_df, start_spark_session, load_df, write_to_excel, get_module_from_path\n",
    "import config as cfg\n",
    "\n",
    "def drop_garbage_cols(df):\n",
    "    \"\"\"\n",
    "    Drop some of the unnesessary columns\n",
    "    \"\"\"\n",
    "    columns_to_drop = ['level_0', 'index', 'Unnamed: 0', '_c0', 'party_id', 'event_date', 'CTU', 'event_id']\n",
    "    df_to_drop = df.select('*')\n",
    "    df_to_drop = df_to_drop.drop(*columns_to_drop)\n",
    "    \n",
    "    return df_to_drop\n",
    "\n",
    "\"\"\"\n",
    "*** MAIN ***\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "spark = start_spark_session()\n",
    "prepro_df  = load_df(cfg.PREPROCESS_PATH)\n",
    "num_rows = prepro_df.count()\n",
    "event_rate_df = prepro_df.select([(F.count(F.when(prepro_df[c] != 0, c))/num_rows).alias(c) for c in prepro_df.columns])\n",
    "event_rate_df_clean =  drop_garbage_cols( event_rate_df)\n",
    "event_rate_df_clean_pd = event_rate_df_clean.toPandas().transpose().reset_index().rename(columns={0:'Column event rate ', 'index' : 'Column names'})\n",
    "event_rate_df_clean_spark = spark.createDataFrame(event_rate_df_clean_pd)\n",
    "write_to_excel(event_rate_df_clean_spark, \"zone_5_stage_I_aggrega_step_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
