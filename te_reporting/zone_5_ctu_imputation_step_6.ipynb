{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imputed_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f22272ab99a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mimputed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_imputed_df\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPUTATION_TRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPUTATION_PREDICT_PATH\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mimputed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputed_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0mimputed_columns_with_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_descriptive_statistics_for_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputed_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mimputed_cols_stats_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mimputed_columns_with_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'column'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imputed_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Zone 5, step 6: CTU imputations\n",
    "\n",
    "After imputing rows for missing CTUs, create table with row-wise listing:\t\t\n",
    "Column name\t\tTable 1\n",
    "Imputation approach (min, max, median, cumsum, zero, FFill, etc.)\t\tTable 1\n",
    "Proportion of accounts that have more than:\t\tTable 1\n",
    "99% missing\t\t\n",
    "75% missing\t\t\n",
    "50% missing\t\t\n",
    "25% missing\t\t\n",
    "\t\t\n",
    "Differences in descriptive statistics between steps 6 and 3\t\tTable 2\n",
    "Delta min\t\t\n",
    "Delta max\t\t\n",
    "Delta mean\t\t\n",
    "Delta std\t\t\n",
    "Delta median\t\n",
    "\n",
    "Input files: \n",
    "imputed_train_ and  preprocessing_\n",
    "\t\t\n",
    "        \n",
    "TO DO:\n",
    "Sort by delta max desc\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from helper_functions import get_imputed_df, start_spark_session, load_df, write_to_excel, get_module_from_path\n",
    "import config as cfg\n",
    "\n",
    "\n",
    "\n",
    "def calc_column_func(df, column, func):\n",
    "\n",
    "    \"\"\"\n",
    "    for a column, calculate a statistical value\n",
    "    \"\"\"\n",
    "\n",
    "    return df.agg({column : func}).collect()[0][0]\n",
    "\n",
    "\n",
    "def get_descriptive_statistics_for_columns(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the columns names and for every column create a tupel col, maximum , minumium to make it sutable to create a datafrma out out tuples\n",
    "    (event1, 3, 1) \n",
    "    \"\"\"\n",
    "\n",
    "    columns = preprocessing_df.schema.names\n",
    "    columns_with_stats = []  # append tuples to a list, later to create a spark df\n",
    "    for col in columns: # for each column calculate stat values\n",
    "        maximum = calc_column_func(df, col, 'max')\n",
    "        minimum = calc_column_func(df, col, 'min')\n",
    "        mean = calc_column_func(df, col, 'avg')\n",
    "        columns_with_stats.append((col,maximum, minimum, mean))\n",
    "    return columns_with_stats \n",
    "\n",
    "\n",
    "def drop_garbage_cols(df):\n",
    "    \"\"\"\n",
    "    Drop some of the unnesessary columns\n",
    "    \"\"\"\n",
    "    columns_to_drop = ['level_0', 'index', 'Unnamed: 0', '_c0']\n",
    "    df_to_drop = df.select('*')\n",
    "    df_to_drop = df_to_drop.drop(*columns_to_drop)\n",
    "    \n",
    "    return df_to_drop\n",
    "\n",
    "\n",
    "def get_delta_columns_df(joined_df):\n",
    "    \"\"\"\n",
    "    Substract simmilar summary columns (like min, max, mean .. ) for preprocessing df and imputed df \n",
    "    \"\"\"\n",
    "    joined_df_min = joined_df.withColumn(\"delta_min\", col(\"min_pre\") - col(\"min\"))\n",
    "    joined_df_min_max = joined_df_min.withColumn(\"delta_max\", col(\"max_pre\") - col(\"max\"))\n",
    "    joined_df_min_max_mean = joined_df_min_max.withColumn(\"delta_mean\", col(\"mean_pre\") - col(\"mean\"))\n",
    "    \n",
    "    return joined_df_min_max_mean\n",
    "    \n",
    " \n",
    "\"\"\"\n",
    "**** MAIN *****\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "spark = start_spark_session()\n",
    "\n",
    "\n",
    "preprocessing_df = load_df(cfg.PREPROCESS_PATH)\n",
    "preprocessing_columns_with_stats = get_descriptive_statistics_for_columns(preprocessing_df)\n",
    "preprocessing_cols_stats_df = spark.createDataFrame( preprocessing_columns_with_stats, ['column','max','min','mean'] )\n",
    "\n",
    "imputed_df = get_imputed_df( cfg.IMPUTATION_TRAIN_PATH, cfg.IMPUTATION_PREDICT_PATH )\n",
    "imputed_columns_with_stats = get_descriptive_statistics_for_columns(imputed_df)\n",
    "imputed_cols_stats_df = spark.createDataFrame( imputed_columns_with_stats, ['column','max','min','mean'] )\n",
    "\n",
    "\n",
    "preprocessing_cols_stats_df_re = preprocessing_cols_stats_df.\\\n",
    "select(*(col(x).alias(x + '_pre') for x in preprocessing_cols_stats_df.columns))\n",
    "joined_df = preprocessing_cols_stats_df_re.join(imputed_cols_stats_df, preprocessing_cols_stats_df_re.column_pre == imputed_cols_stats_df.column)\n",
    "\n",
    "delta_columns_df = get_delta_columns_df(joined_df)\n",
    "delta_columns_df.select('column','delta_min', 'delta_max', 'delta_mean').show(n=45, truncate= False)\n",
    "write_to_excel(delta_columns_df, \"zone_5_ctu_imputation_step_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------+--------------------+----------------------+\n",
      "|column                      |delta_min|delta_max           |delta_mean            |\n",
      "+----------------------------+---------+--------------------+----------------------+\n",
      "|expanding_cai_ins_grs_mrc   |0.0      |0.0                 |242.164572469429      |\n",
      "|yr_month                    |0.0      |2.0                 |8.248874504497508     |\n",
      "|event_inbound_interactions  |0.0      |0.0                 |0.0038466376698513827 |\n",
      "|month                       |0.0      |0.0                 |-0.006062903445177348 |\n",
      "|td_last_cai_ins_grs_vmc     |0.0      |0.0                 |0.0014165845494440443 |\n",
      "|td_last_cai_ins_grs_vuc     |0.0      |0.0                 |5.114062678986507     |\n",
      "|cai_ins_grs_mrc             |0.0      |0.0                 |-0.016975163022877382 |\n",
      "|cai_ins_grs_rand            |0.0      |0.0                 |6.496808137061365E-4  |\n",
      "|CTU                         |-1.0     |0.0                 |-0.9314790226141447   |\n",
      "|event_date                  |null     |null                |null                  |\n",
      "|td_last_cai_ins_grs_erc     |0.0      |0.0                 |-2.4420860151980195E-4|\n",
      "|event_outbound_interactions |0.0      |0.0                 |1.2890040616775345E-5 |\n",
      "|cai_factor_1                |0.0      |0.0                 |-0.0021779610931655924|\n",
      "|expanding_cai_ins_grs_evnt_2|0.0      |0.0                 |0.5474122253825984    |\n",
      "|expanding_cai_ins_grs_vuc   |0.0      |0.0                 |13.795573054983024    |\n",
      "|td_last_cai_ins_grs_mrc     |0.0      |0.0                 |-0.0015178609202317617|\n",
      "|_c0                         |0.0      |0.0                 |4994.5                |\n",
      "|event_id                    |0.0      |0.0                 |0.0                   |\n",
      "|event_positive_interactions |0.0      |0.0                 |-0.13380396374365944  |\n",
      "|REF_EVENT                   |0.0      |0.0                 |-3.307893497912426E-4 |\n",
      "|expanding_cai_ins_grs_evnt_3|0.0      |0.0                 |0.5571035565200635    |\n",
      "|td_last_cai_ins_grs_evnt_1  |0.0      |0.0                 |1.0199512491376908    |\n",
      "|cai_ins_grs_vmc             |0.0      |0.0                 |0.005016838034636706  |\n",
      "|td_last_cai_ins_grs_evmc    |0.0      |0.0                 |1.5745063779519413    |\n",
      "|cai_ins_grs_evnt_3          |0.0      |0.0                 |1.2890040616775345E-5 |\n",
      "|year                        |0.0      |0.0                 |0.0825493740796901    |\n",
      "|cai_ins_grs_erc             |0.0      |0.0                 |-0.06674347277629522  |\n",
      "|index                       |0.0      |9000.0              |18.61802231874026     |\n",
      "|td_start                    |0.0      |0.0                 |29.929629511830797    |\n",
      "|cai_factor_age              |0.0      |0.0                 |0.007598058771783656  |\n",
      "|Unnamed: 0                  |0.0      |9000.0              |18.61802231874026     |\n",
      "|expanding_cai_ins_grs_erc   |0.0      |0.0                 |256.942681166714      |\n",
      "|expanding_cai_ins_grs_evmc  |0.0      |1.313431368594138   |33.57592104728468     |\n",
      "|party_id                    |0.0      |0.0                 |0.18250748470677536   |\n",
      "|td_last_cai_ins_grs_evnt_2  |0.0      |0.0                 |1.3326945108697643    |\n",
      "|cai_ins_grs_evnt_2          |0.0      |0.0                 |-7.371406983479462E-4 |\n",
      "|expanding_cai_ins_grs_vmc   |0.0      |0.0                 |46.98051914524808     |\n",
      "|cai_ins_grs_vuc             |0.0      |0.022501208513475035|-0.02174924068333528  |\n",
      "|td_last_cai_ins_grs_evnt_3  |0.0      |0.0                 |1.0090488645250133    |\n",
      "|cai_factor_2                |0.0      |0.0                 |0.0                   |\n",
      "|cai_ins_grs_evmc            |0.0      |0.0                 |-0.0551021659790667   |\n",
      "|cai_ins_grs_evnt_1          |0.0      |0.0                 |0.004583778368199454  |\n",
      "|level_0                     |0.0      |9000.0              |18.61802231874026     |\n",
      "|event_negative_interactions |0.0      |0.0                 |0.0                   |\n",
      "|expanding_cai_ins_grs_evnt_1|0.0      |756.0               |23.435950706747207    |\n",
      "+----------------------------+---------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputed_columns_with_stats = get_descriptive_statistics_for_columns(imputed_df)\n",
    "imputed_cols_stats_df = spark.createDataFrame( imputed_columns_with_stats, ['column','max','min','mean'] )\n",
    "\n",
    "\n",
    "preprocessing_cols_stats_df_re = preprocessing_cols_stats_df.\\\n",
    "select(*(col(x).alias(x + '_pre') for x in preprocessing_cols_stats_df.columns))\n",
    "joined_df = preprocessing_cols_stats_df_re.join(imputed_cols_stats_df, preprocessing_cols_stats_df_re.column_pre == imputed_cols_stats_df.column)\n",
    "\n",
    "delta_columns_df = get_delta_columns_df(joined_df)\n",
    "delta_columns_df.select('column','delta_min', 'delta_max', 'delta_mean').show(n=45, truncate= False)\n",
    "write_to_excel(delta_columns_df, \"zone_5_ctu_imputation_step_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_file_name = \"../data/preprocessing_2020_06_30_1.csv\"\n",
    "imputed_file_name = \"../data/imputed_train_2020_06_30_1.csv\"\n",
    "genereal_preprocessing_file_name = \"../data/general_preprocessing_2020_06_30_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = start_spark_session()\n",
    "preprocessing_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(preprocessing_file_name)\n",
    "imputed_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(imputed_file_name)\n",
    "general_preprocessing_df = spark.read.format(\"csv\").option(\"header\", \"true\").\\\n",
    "        load(genereal_preprocessing_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_preprocessing_df.where(\"party_id = 7\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_df.where(\"ctu >= 0\").where(\"party_id = 7\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_preprocessing_pds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_df_pds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_rows', None)\n",
    "general_preprocessing_pds['count'] - preprocessing_df_pds['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_df.where(\"ctu >= 0\").groupBy('party_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.where(\"ctu >=0\").groupBy('party_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_df.where(\"party_id == 7\").select(\"event_date\",\"cai_ins_grs_vmc\",\n",
    "                                               \"cai_ins_grs_mrc\",\"cai_ins_grs_erc\", \"ctu\").show(n=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.where(\"party_id == 7\").select(\"event_date\",\"cai_ins_grs_vmc\",\n",
    "                                               \"cai_ins_grs_mrc\",\"cai_ins_grs_erc\", \"ctu\").show(n=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_preprocessing_df.where(\"party_id == 7\").select(\"event_date\",\"cai_ins_grs_vmc\",\"cai_ins_grs_mrc\",\"cai_ins_grs_erc\").count()\n",
    "#.show(n=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
