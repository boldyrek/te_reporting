{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+----+----+\n",
      "| event|event_stats|   0|   1|   2|\n",
      "+------+-----------+----+----+----+\n",
      "|   CTU|        max|   0|   1|   2|\n",
      "|   CTU|        min|   0|   1|   2|\n",
      "|   CTU|       mean| 0.0| 1.0| 2.0|\n",
      "|   CTU|     stddev| 0.0| 1.0| 2.0|\n",
      "|event1|     stddev| 6.0| 2.0| 3.0|\n",
      "|event1|        max|   6|   3|   4|\n",
      "|event1|       mean| 6.0| 2.0| 3.0|\n",
      "|event1|        min|   6|   1|   2|\n",
      "|event2|        min|  15|  10|  11|\n",
      "|event2|       mean|15.0|11.0|12.0|\n",
      "|event2|        max|  15|  12|  13|\n",
      "|event2|     stddev|15.0|11.0|12.0|\n",
      "| party|         id|   0|   2|   2|\n",
      "| party|         id|   0|   1|   1|\n",
      "| party|         id| 0.0| 1.5| 1.5|\n",
      "| party|         id| 0.0| 1.5| 1.5|\n",
      "+------+-----------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Zone 5 , step 5: ctu report\n",
    "For every CTU we get min, max, mean, stddev ... summary statistics\n",
    "\n",
    "\tCreate a table with row-wise listing of:\t\t\t\n",
    "1\tCTU\t\t\tTable 1\n",
    "2\tMin and max event dates in each CTU\t\t\tTable 1\n",
    "3\tLength of each CTU in days\t\t\tTable 2\n",
    "4\tEvent descriptive stats across CTUs and all batches\t\t\tTable 1\n",
    "\tMinimum\t\t\t\n",
    "\tMaximum\t\t\t\n",
    "\tMean\t\t\t\n",
    "5\tTarget event stats across CTUs and all batches:\t\t\tTable 1\n",
    "\tMinimum\t\t\t\n",
    "\tMaximum\t\t\t\n",
    "\tMean\t\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from col_stats import *\n",
    "\n",
    "def get_spark_session():\n",
    "    \"\"\"\n",
    "    Starting spark session\n",
    "    \"\"\"\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_event_stats_list_to_df(event_stats_list, CTU_num):\n",
    "    \"\"\"\n",
    "    creata a df:\n",
    "    colmn_name stats_summary_name stats_summary_value\n",
    "    event 1       min             1\n",
    "    event 1       max             5\n",
    "    event 2       min             101\n",
    "    event 2       max             199\n",
    "    ......\n",
    "    based on input of list of typles like [(event1, min, 1), (event1, max, 5)]\n",
    "    \"\"\"\n",
    "    \n",
    "    cols_stats_df = spark.createDataFrame(\\\n",
    "                                          event_stats_list,\\\n",
    "                                          [ 'event_plus_summary',  str(CTU_num) ] )\n",
    "    \n",
    "    return cols_stats_df \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_list_of_ctu_event_stats(df):\n",
    "    \"\"\"\n",
    "    Input a dataframe\n",
    "    Output a list with stats for every columns [(column1, min, 1), (column1, max, 5), (column2, min,     105), ... ]\n",
    "    \"\"\"\n",
    "    \n",
    "    all_columns_summary_stats = []\n",
    "    columns = df.schema.names\n",
    "    for col in columns: # for each column calculate stat values\n",
    "        df_one_col = df.select(col)\n",
    "        maximum = calc_column_max( df_one_col )\n",
    "        maximum_row = (col+'_max', str(maximum))\n",
    "        all_columns_summary_stats.append( maximum_row )\n",
    "        \n",
    "        minimum =  calc_column_min( df_one_col )\n",
    "        minimum_row = (col +'_min', str(minimum))\n",
    "        all_columns_summary_stats.append( minimum_row )\n",
    "        \n",
    "        mean = calc_column_avg( df_one_col )\n",
    "        mean_row = (col + '_mean', str(mean))\n",
    "        all_columns_summary_stats.append( mean_row )\n",
    "        \n",
    "        stddev = calc_column_stddev( df_one_col )\n",
    "        stddev_row = (col + '_stddev', str(mean))\n",
    "        all_columns_summary_stats.append( stddev_row )\n",
    "        \n",
    "    return all_columns_summary_stats\n",
    "\n",
    "\n",
    "\n",
    "def get_df_with_dropped_garbage_cols(df):\n",
    "    \"\"\"\n",
    "    Drop some of the unnesessary columns\n",
    "    \"\"\"\n",
    "    columns_to_drop = ['level_0', 'index', 'Unnamed: 0', '_c0']\n",
    "    df_to_drop = df.select('*')\n",
    "    df_to_drop = df_to_drop.drop(*columns_to_drop)\n",
    "    \n",
    "    return df_to_drop\n",
    "\n",
    "\n",
    "\n",
    "def get_joined_df( event_stats_all_ctus ):\n",
    "    \"\"\"\n",
    "    Input: getting a list of dataframes for every CTU: 0,1,2,3 .. max_CTU \n",
    "    need to join them together in a loop along the axis 1 \n",
    "    return a df with columns\n",
    "    column_name stats_summary_name stats_summary_value\n",
    "    We need to join like  this looking dataframes into one with all the CTUs:\n",
    "      +-------------------+----+\n",
    "        |event_plus_summary|   1|\n",
    "        +-------------------+----+\n",
    "        |         event1_max|   3|\n",
    "        |         event1_min|   1|\n",
    "        |        event1_mean| 2.0|\n",
    "        |      event1_stddev| 2.0|\n",
    "    \"\"\"\n",
    "    ctu_summary_joined_df = event_stats_all_ctus[0].select(\"*\") # get first ctu df as a starting poin for joining\n",
    "    for ctu_num , ctu_summary_stats in enumerate(event_stats_all_ctus[1:]):\n",
    "        ctu_summary_joined_df = ctu_summary_joined_df.join(\n",
    "                ctu_summary_stats, ctu_summary_joined_df.event_plus_summary \\\n",
    "                == ctu_summary_stats.event_plus_summary).\\\n",
    "                drop(ctu_summary_stats.event_plus_summary)\n",
    "    #ctu_summary_joined_df.show().column_name == ctu_summary_stats.column_name)\n",
    "    return ctu_summary_joined_df\n",
    "\n",
    "def get_event_stats_df( single_CTU_clean_df, CTU_num ):\n",
    "        \"\"\"\n",
    "        Output df looks like this:\n",
    "        +-------------------+----+\n",
    "        |event_plus_summary|   1|\n",
    "        +-------------------+----+\n",
    "        |         event1_max|   3|\n",
    "        |         event1_min|   1|\n",
    "        |        event1_mean| 2.0|\n",
    "        |      event1_stddev| 2.0|\n",
    "        \"\"\"\n",
    "        event_stats =  get_list_of_ctu_event_stats(  single_CTU_clean_df ) # getting a list summary stats\n",
    "        event_stats_df = transform_event_stats_list_to_df ( event_stats, CTU_num )\n",
    "        return event_stats_df\n",
    "        \n",
    "def read_file_into_df( spark, file_name ):\n",
    "    \n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_name)\n",
    "    return df\n",
    "\n",
    "def print_df (joined_ctu_event_stats_df ):\n",
    "    joined_ctu_event_stats_df.select(\n",
    "        ['event_plus_summary', '0', '1', '2']).show(200,truncate =False)\n",
    "\n",
    "def get_max_CTU_num(df):\n",
    "    \n",
    "    df_CTU = df.select('CTU')\n",
    "    max_CTU = calc_column_max( df_CTU ) # get the maximum values of a CTU, to determin num of CTUS\n",
    "    max_CTU_int = int(float( max_CTU )) # convert string values like 9.0 into int \n",
    "    \n",
    "    return max_CTU_int\n",
    "\n",
    "def split_column_by_underscore(df):\n",
    "    \"\"\"\n",
    "    Split columns_plus_summary column by undescore and \n",
    "    remove splited column \n",
    "    \"\"\"\n",
    "    split_col = pyspark.sql.functions.split(df['event_plus_summary'], '_')\n",
    "    df = df.withColumn('event', split_col.getItem(0))\n",
    "    df = df.withColumn('event_stats', split_col.getItem(1))\n",
    "    df = df.drop('event_plus_summary')\n",
    "    \n",
    "    return df \n",
    "\n",
    "def get_event_cols_first ( joined_ctu_event_stats_df ):\n",
    "    \"\"\"\n",
    "    put event event columns first \n",
    "    \"\"\"\n",
    "    cols = joined_ctu_event_stats_df.columns\n",
    "    new_order_cols =  cols[-2:] + cols[:-2]\n",
    "    return new_order_cols\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "***** MAIN *******\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "file_name = \"../data/example.csv\"\n",
    "#file_name = \"../data/imputed_predict_2020_06_30_1.csv\"\n",
    "spark = get_spark_session(  )\n",
    "df = read_file_into_df( spark, file_name )\n",
    "event_stats_ctus_dfs = []\n",
    "max_CTU_num = get_max_CTU_num(df)\n",
    "for CTU_num in range(max_CTU_num):\n",
    "    \n",
    "    single_CTU_df = df.filter(f\"CTU == { CTU_num }\") # get a df with one CTU\n",
    "    single_CTU_clean_df = get_df_with_dropped_garbage_cols( single_CTU_df )\n",
    "    ctu_event_stats_df = get_event_stats_df( single_CTU_clean_df, CTU_num )\n",
    "    event_stats_ctus_dfs.append( ctu_event_stats_df )\n",
    "\n",
    "\n",
    "joined_ctu_event_stats_df = get_joined_df( event_stats_ctus_dfs )\n",
    "#print_df (joined_ctu_event_stats_df)\n",
    "joined_ctu_event_stats_df = split_column_by_underscore( joined_ctu_event_stats_df )\n",
    "new_order_cols = get_event_cols_first( joined_ctu_event_stats_df)\n",
    "joined_ctu_event_stats_df.select(new_order_cols).orderBy('event').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
