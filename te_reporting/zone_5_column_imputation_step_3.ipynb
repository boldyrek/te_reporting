{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------------------+------+\n",
      "|              column|                 max|                 min|                mean|            stddev|median|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+------+\n",
      "|                 _c0|                9999|                   0|             45659.0|26361.668953235872|     0|\n",
      "|             level_0|                9999|                   0|             45659.0|26361.668953235872|     0|\n",
      "|               index|                9999|                   0|             45659.0|26361.668953235872|     0|\n",
      "|          Unnamed: 0|                9999|                   0|             45659.0|26361.668953235872|     0|\n",
      "|            party_id|                 999|                   1|  501.15303496534125|288.93613666356515|     0|\n",
      "|          event_date|          2020-06-29|          2019-01-01|                null|              null|     0|\n",
      "|     cai_ins_grs_vmc|                 9.0|                 0.0|   9.329460462773355| 7.106271137047644|     0|\n",
      "|     cai_ins_grs_mrc|                99.0|                 0.0|  48.154929423230655| 36.18869539445268|     0|\n",
      "|     cai_ins_grs_erc|                99.0|                 0.0|  51.234682815186325| 35.38973708290102|     0|\n",
      "|    cai_ins_grs_evmc|   99.97842227910921|                 0.0|   6.692393058868284| 24.49151215719403|     0|\n",
      "|     cai_ins_grs_vuc|   99.94141017977616|                 0.0|   2.709403221370962|14.902691938407514|     0|\n",
      "|  cai_ins_grs_evnt_1|                  42|                   0|   4.617199049485868| 13.13794973183203|     0|\n",
      "|  cai_ins_grs_evnt_2|                   1|                   0| 0.10961574261654201|0.3124119081936186|     0|\n",
      "|  cai_ins_grs_evnt_3|                   1|                   0| 0.10961574261654201|0.3124119081936188|     0|\n",
      "|    cai_ins_grs_rand|9.369986777308052...|-0.00013197381791...|-0.00270620386022...|1.0005358035941219|     0|\n",
      "|      cai_factor_age|                59.0|                 0.0|   39.44749723496753| 11.75275583940931|     0|\n",
      "|        cai_factor_1|                 8.0|                 0.0|  3.8976007183609105| 2.540948863174911|     0|\n",
      "|        cai_factor_2|                   0|                   0|                 0.0|               0.0|     0|\n",
      "|            event_id|                   1|                   1|                 1.0|               0.0|     0|\n",
      "|expanding_cai_ins...|               999.0|                 0.0|   433.7966140671711| 256.7367548915836|     0|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Te version 2.7\n",
    "Zone 5, step 3:  column imputation\n",
    "\t\t\t\t\t\t\t\n",
    "\tTask description\t\t\t\t\t\tAutomation\n",
    "\tAfter imputing at the raw column level, create table with row-wise listing of:\t\t\t\t\t\t\n",
    "1\tColumn name\t\t\t\t\t\tTable 1\n",
    "2\tImputation approach (Te version 2.7 implements only zero of forward fill)\t\t\t\t\t\tTable 1\n",
    "3\tColumn descriptive statistics:\t\t\t\t\t\tTable 1\n",
    "\tminimum\t\t\t\t\t\t\n",
    "\tmaximum\t\t\t\t\t\t\n",
    "\tmean\t\t\t\t\t\t\n",
    "\tstandard deviation\t\t\t\t\t\t\n",
    "\tMedian\t\t\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "Input:  General preprocessing dataframe\n",
    "Output: of the script should be in excell format and should looks like table bellow:\n",
    "         minimum\tmaximum\tmean\tstandard deviation\tmedian\timputation approach\n",
    "Column 1\t1\t     13\t     7\t      4\t       8\t     5            ffill\n",
    "Column 2\t1\t     6\t     4\t      2\t       4\t     3            bfill\n",
    "Column 3\t101\t     112\t 106\t  4\t       107       7            bfill \t\n",
    "…\t…\t…\t…\t…\t…\t\n",
    "\"\"\"\n",
    "\n",
    "from sys import argv\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def start_spark_session():\n",
    "    \"\"\"\n",
    "    Starting spark session\n",
    "    \"\"\"\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark \n",
    "\n",
    "\n",
    "def calc_summary_func(df, column, func):\n",
    "    \"\"\"\n",
    "    for a column, calculate a statistical function\n",
    "    \"\"\"\n",
    "\n",
    "    if func == 'median':\n",
    "        #summary_value  = df.approxQuantile(column, [0.5], 0.25) # need to verify this function\n",
    "        summary_value = 0 # temporary zero. will ber removed when verify function above  \n",
    "    else: \n",
    "        summary_value = df.agg({column : func}).collect()[0][0]\n",
    "    return summary_value\n",
    "\n",
    "\n",
    "def get_summary_stats_for_every_column(df, columns):\n",
    "    \"\"\"\n",
    "    Input: Input df, and columns of that dataframe\n",
    "    Calculate summary statistics for every column \n",
    "    Output: \n",
    "    \"\"\"\n",
    "   \n",
    "    columns_summary_stats = [] # append tuples to a list, later to create a spark df\n",
    "    for col in columns: # for each column calculate stat values\n",
    "        maximum = calc_summary_func(df, col, 'max')\n",
    "        minimum = calc_summary_func(df, col, 'min')\n",
    "        mean = calc_summary_func(df, col, 'avg')\n",
    "        stddev = calc_summary_func(df, col, 'stddev')\n",
    "        median = calc_summary_func(df, col, 'median')\n",
    "      \n",
    "        columns_summary_stats.append((col,maximum, minimum, mean, stddev, median))\n",
    "    \n",
    "    return columns_summary_stats\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "****** MAIN ******\n",
    "1. Create spark session \n",
    "2. Read the file into a dataframe\n",
    "3. a. Calculate statistical summary for every column in a dataframe\n",
    "   b. Get imputation approach from the te_constants.py ?\n",
    "4. Create a new dataframe that has columns as rows and stats summaries as values\n",
    "5. Save it as an excel tab \n",
    "\"\"\"\n",
    "\n",
    "#file_name = \"../example.csv\"\n",
    "file_name = \"../general_preprocessing_2020_06_30_1.csv\"\n",
    "spark = start_spark_session()\n",
    "gen_pre_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_name) \n",
    "columns = gen_pre_df.schema.names\n",
    "columns_summary_stats = get_summary_stats_for_every_column(gen_pre_df, columns)\n",
    "excel_ready_df = spark.createDataFrame( columns_summary_stats, ['column', 'max', 'min', 'mean', 'stddev', 'median' ] )\n",
    "excel_ready_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
