{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+------------------+------------------+------+\n",
      "|  column|max|min|              mean|            stddev|median|\n",
      "+--------+---+---+------------------+------------------+------+\n",
      "|  event1|  6|  1|               3.5|1.8708286933869707|   2.0|\n",
      "|  event2| 15| 10|              12.5|1.8708286933869707|  11.0|\n",
      "|party_id|  2|  0|1.3333333333333333| 0.816496580927726|   1.0|\n",
      "|     CTU|  3|  0|               1.5|1.0488088481701516|   1.0|\n",
      "+--------+---+---+------------------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Te version 2.7\n",
    "Zone 5, step 3:  column imputation\n",
    "\t\t\t\t\t\t\t\n",
    "\tTask description\t\t\t\t\t\tAutomation\n",
    "\tAfter imputing at the raw column level, create table with row-wise listing of:\t\t\t\t\t\t\n",
    "1\tColumn name\t\t\t\t\t\tTable 1\n",
    "2\tImputation approach (Te version 2.7 implements only zero of forward fill)\t\t\t\t\t\tTable 1\n",
    "3\tColumn descriptive statistics:\t\t\t\t\t\tTable 1\n",
    "\tminimum\t\t\t\t\t\t\n",
    "\tmaximum\t\t\t\t\t\t\n",
    "\tmean\t\t\t\t\t\t\n",
    "\tstandard deviation\t\t\t\t\t\t\n",
    "\tMedian\t\t\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "Input:  General preprocessing dataframe\n",
    "Output: of the script should be in excell format and should looks like table bellow:\n",
    "         minimum\tmaximum\tmean\tstandard deviation\tmedian\timputation approach\n",
    "Column 1\t1\t     13\t     7\t      4\t       8\t     5            ffill\n",
    "Column 2\t1\t     6\t     4\t      2\t       4\t     3            bfill\n",
    "Column 3\t101\t     112\t 106\t  4\t       107       7            bfill \t\n",
    "…\t…\t…\t…\t…\t…\t\n",
    "\"\"\"\n",
    "\n",
    "from sys import argv\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from col_stats import *\n",
    "\n",
    "\n",
    "\n",
    "def start_spark_session():\n",
    "    \"\"\"\n",
    "    Starting spark session\n",
    "    \"\"\"\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_summary_stats_for_every_column(df):\n",
    "    \"\"\"\n",
    "    Input: Input df, and columns of that dataframe\n",
    "    Calculate summary statistics for every column \n",
    "    Output: \n",
    "    \n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    columns_summary_stats = [] # append tuples to a list, later to create a spark df\n",
    "    for col in columns: # for each column calculate stat values\n",
    "        one_col_df = df.select(col) # select only nesessary colum\n",
    "        maximum = calc_column_max(one_col_df)\n",
    "#         print(maximum)\n",
    "#         raise SystemExit\n",
    "        minimum = calc_column_min(one_col_df)\n",
    "        mean = calc_column_avg(one_col_df)\n",
    "        stddev = calc_column_stddev(one_col_df)\n",
    "        median = calc_column_median(one_col_df)\n",
    "        columns_summary_stats.append((col, maximum, minimum, mean, stddev, median))\n",
    "    \n",
    "    return columns_summary_stats\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "****** MAIN ******\n",
    "1. Create spark session \n",
    "2. Read the file into a dataframe\n",
    "3. a. Calculate statistical summary for every column in a dataframe\n",
    "   b. Get imputation approach from the te_constants.py ?\n",
    "4. Create a new dataframe that has columns as rows and stats summaries as values\n",
    "5. Save it as an excel tab \n",
    "\"\"\"\n",
    "\n",
    "file_name = \"../data/example.csv\"\n",
    "#file_name = \"../data/general_preprocessing_2020_06_30_1.csv\"\n",
    "spark = start_spark_session()\n",
    "gen_pre_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_name) \n",
    "\n",
    "columns_summary_stats = get_summary_stats_for_every_column(gen_pre_df)\n",
    "excel_ready_df = spark.createDataFrame( columns_summary_stats,\\\n",
    "                                       ['column', 'max', 'min', 'mean', 'stddev', 'median' ] )\n",
    "excel_ready_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to work with only one column in spark so we can simplify functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
